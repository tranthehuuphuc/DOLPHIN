## dolphin_patterns.py
# This file defines DOLPHIN patterns and implements the greedy matching algorithm

# DOLPHIN patterns definition
DOLPHIN_PATTERNS = {
    "vowels": {
        1: ["a", "e", "i", "o", "u"],
        2: ["ai", "al", "ar", "au", "aw", "ay", "ea", "ee", "ei", "er", "eu", "ew", "ey", "ia", "ie", "ir", "oa", "oe", "oi", "oo", "or", "ou", "ow", "oy", "ue", "ui", "ur"],
        3: ["air", "ear", "eer", "igh", "ign", "ing", "ion", "oew", "ore", "our", "ure"]
    },
    "consonants": {
        1: ["b", "c", "d", "f", "g", "h", "j", "k", "l", "m", "n", "p", "q", "r", "s", "t", "v", "w", "x", "y", "z"],
        2: ["bl", "br", "ch", "ck", "cl", "cr", "dr", "fl", "fr", "gh", "gl", "gr", "kn", "ld", "lk", "mb", "mn", "mp", "nd", "ng", "nk", "nt", "ph", "pl", "pn", "pr", "ps", "qe", "qu", "rh", "sc", "sh", "sk", "sl", "sm", "sn", "sp", "st", "sw", "th", "tr", "wh", "wr"],
        3: ["dge", "gue", "nch", "que", "shr", "spl", "spr", "squ", "str", "tch", "thr"]
    }
}

def greedy_match(domain_name, patterns):
    """
    Splits a domain name into DOLPHIN patterns using a greedy matching algorithm.

    Args:
        domain_name (str): The domain name to process.
        patterns (dict): The DOLPHIN patterns for vowels and consonants.

    Returns:
        list: A list of tuples (matched substring, its type).
    """
    i = 0  # Start index
    length = len(domain_name)
    result = []

    while i < length:
        match_found = False

        # Try to match trigraphs, digraphs, and single characters in order
        for size in [3, 2, 1]:
            if i + size > length:
                continue

            substring = domain_name[i:i + size]

            # Check for vowels
            if substring in patterns["vowels"].get(size, []):
                result.append((substring, "vowel"))
                i += size
                match_found = True
                break

            # Check for consonants
            elif substring in patterns["consonants"].get(size, []):
                result.append((substring, "consonant"))
                i += size
                match_found = True
                break

        # Handle unmatched characters (neutral)
        if not match_found:
            result.append((domain_name[i], "neutral"))
            i += 1

    return result

## feature_extraction.py
# This file computes features based on DOLPHIN patterns

def compute_phonics_features(matches):
    """
    Computes phonics-based features from matched patterns.

    Args:
        matches (list): List of tuples (substring, type).

    Returns:
        dict: Phonics-based features.
    """
    vowels = [item for item in matches if item[1] == "vowel"]
    consonants = [item for item in matches if item[1] == "consonant"]
    total_length = len(matches)

    vowel_ratio = len(vowels) / total_length if total_length > 0 else 0
    repeated_chars = len(set(item[0] for item in matches if matches.count(item) > 1)) / total_length
    consecutive_consonants = sum(1 for i in range(len(consonants) - 1)
                                 if consonants[i][0] + consonants[i + 1][0]) / total_length

    return {
        "vowel_ratio": vowel_ratio,
        "repeated_chars": repeated_chars,
        "consecutive_consonants": consecutive_consonants
    }

## preprocess.py
# Handles data preprocessing

import pandas as pd

def preprocess_dataset(file_path, whitelist):
    """
    Reads a dataset from a CSV file and preprocesses it.

    Args:
        file_path (str): Path to the CSV file.
        whitelist (set): Set of whitelisted domain names to exclude.

    Returns:
        pd.DataFrame: Cleaned dataset.
    """
    data = pd.read_csv(file_path)

    # Validate columns
    if "domain_name" not in data.columns or "label" not in data.columns:
        raise ValueError("Dataset must contain 'domain_name' and 'label' columns.")

    # Drop invalid rows
    data.dropna(subset=["domain_name", "label"], inplace=True)
    data = data[data["domain_name"].str.contains("\\.")]

    # Exclude whitelisted domains
    data = data[~data["domain_name"].isin(whitelist)]

    return data

## model_training.py
# Handles training and evaluation of models

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

def train_and_evaluate_model(features):
    """
    Trains and evaluates a Random Forest model.

    Args:
        features (pd.DataFrame): Features and labels.

    Returns:
        RandomForestClassifier: Trained model.
    """
    X = features.drop(columns=["label"])
    y = features["label"]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)

    predictions = model.predict(X_test)
    print(classification_report(y_test, predictions))

    return model

## main.py
# Entry point for the pipeline

from preprocess import preprocess_dataset
from dolphin_patterns import greedy_match, DOLPHIN_PATTERNS
from feature_extraction import compute_phonics_features
from model_training import train_and_evaluate_model

def main():
    dataset_path = "path/to/dataset.csv"
    whitelist_path = "path/to/whitelist.txt"

    # Load whitelist domains
    with open(whitelist_path, "r") as file:
        whitelist = set(domain.strip() for domain in file.readlines())

    data = preprocess_dataset(dataset_path, whitelist)

    features = []
    for _, row in data.iterrows():
        matches = greedy_match(row["domain_name"], DOLPHIN_PATTERNS)
        phonics_features = compute_phonics_features(matches)
        phonics_features["label"] = row["label"]
        features.append(phonics_features)

    features_df = pd.DataFrame(features)
    train_and_evaluate_model(features_df)

if __name__ == "__main__":
    main()

## tests/test_patterns.py
# Test cases for dolphin_patterns.py

from dolphin_patterns import greedy_match, DOLPHIN_PATTERNS

def test_greedy_match():
    domain = "nationalgeographic"
    matches = greedy_match(domain, DOLPHIN_PATTERNS)
    expected = [
        ('n', 'consonant'), ('a', 'vowel'), ('t', 'consonant'),
        ('ion', 'vowel'), ('al', 'vowel'), ('g', 'consonant'),
        ('e', 'vowel'), ('o', 'vowel'), ('gr', 'consonant'),
        ('a', 'vowel'), ('ph', 'consonant'), ('i', 'vowel'),
        ('c', 'consonant')
    ]
    assert matches == expected

## requirements.txt
# Required Python libraries
pandas
scikit-learn
pytest

## README.md
# Documentation for the project

# DOLPHIN Project

This project implements the DOLPHIN framework for detecting DGA-based botnets using phonics-based features.

## Structure
- `dolphin_patterns.py`: Defines patterns and matching logic.
- `feature_extraction.py`: Computes features based on DOLPHIN patterns.
- `preprocess.py`: Preprocesses the dataset.
- `model_training.py`: Handles training and evaluation.
- `main.py`: Runs the pipeline.
- `tests/`: Contains test cases.

## Usage
1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Run the pipeline:
   ```bash
   python main.py
   ```

3. Run tests:
   ```bash
   pytest tests/

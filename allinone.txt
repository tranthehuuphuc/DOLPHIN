# main.py

import pandas as pd
import sys
import os

# Ensure the current directory is in the Python path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from preprocess import preprocess_dataset
from dolphin_patterns import greedy_match, DOLPHIN_PATTERNS
from feature_extraction import compute_phonics_features
from model_training import train_and_evaluate_model

def main():
    # Default paths (can be modified as needed)
    dataset_path = "data.csv"
    whitelist_path = "whitelist.txt"

    # Load whitelist domains
    try:
        with open(whitelist_path, "r") as file:
            whitelist = set(domain.strip() for domain in file.readlines())
    except FileNotFoundError:
        print(f"Whitelist file {whitelist_path} not found. Continuing with empty whitelist.")
        whitelist = set()

    # Preprocess the dataset
    try:
        data = preprocess_dataset(dataset_path, whitelist)
    except FileNotFoundError:
        print(f"Dataset file {dataset_path} not found. Please check the file path.")
        return
    except Exception as e:
        print(f"Error processing dataset: {e}")
        return

    # Extract features
    features = []
    for _, row in data.iterrows():
        # Apply DOLPHIN pattern matching
        matches = greedy_match(row["domain_name"], DOLPHIN_PATTERNS)
        
        # Compute phonics features
        phonics_features = compute_phonics_features(matches)
        
        # Add label to features
        phonics_features["label"] = row["label"]
        
        features.append(phonics_features)

    # Convert to DataFrame
    features_df = pd.DataFrame(features)

    # Train and evaluate the model
    train_and_evaluate_model(features_df)

if __name__ == "__main__":
    main()

# dolphin_patterns.py

from typing import List, Dict, Tuple, Set

# DOLPHIN patterns definition (exactly as in the paper)
DOLPHIN_PATTERNS = {
    "vowels": {
        1: ["a", "e", "i", "o", "u"],
        2: ["ai", "al", "ar", "au", "aw", "ay", "ea", "ee", "ei", 
            "er", "eu", "ew", "ey", "ia", "ie", "ir", "oa", "oe", 
            "oi", "oo", "or", "ou", "ow", "oy", "ue", "ui", "ur"],
        3: ["air", "ear", "eer", "igh", "ign", "ing", "ion", "oew", "ore", "our", "ure"]
    },
    "consonants": {
        1: ["b", "c", "d", "f", "g", "h", "j", "k", "l", "m", "n", 
            "p", "q", "r", "s", "t", "v", "w", "x", "y", "z"],
        2: ["bl", "br", "ch", "ck", "cl", "cr", "dr", "fl", "fr", "gh", "gl", "gr", 
            "kn", "ld", "lk", "mb", "mn", "mp", "nd", "ng", "nk", "nt", "ph", "pl", 
            "pn", "pr", "ps", "qe", "qu", "rh", "sc", "sh", "sk", "sl", "sm", "sn", 
            "sp", "st", "sw", "th", "tr", "wh", "wr"],
        3: ["dge", "gue", "nch", "que", "shr", "spl", "spr", "squ", "str", "tch", "thr"]
    }
}

def greedy_match(domain_name: str, patterns: Dict) -> List[Tuple[str, str]]:
    """
    Implement greedy matching algorithm for DOLPHIN patterns.
    
    Args:
        domain_name (str): Domain name to process
        patterns (Dict): Dictionary of DOLPHIN patterns
    
    Returns:
        List of tuples with matched substrings and their types
    """
    matcher = DolphinMatcher(patterns)
    return matcher.construct_output_function(domain_name)

class DolphinMatcher:
    def __init__(self, patterns: Dict):
        """
        Initialize the Dolphin Matcher with predefined patterns.
        
        Args:
            patterns (Dict): Dictionary of DOLPHIN patterns for vowels and consonants
        """
        self.patterns = patterns
        self.max_pattern_length = max(
            len(pattern) 
            for pattern_type in patterns.values() 
            for length_group in pattern_type.values() 
            for pattern in length_group
        )
        
        # Precompute all possible patterns for faster lookup
        self.all_patterns = set()
        for pattern_type in patterns.values():
            for length_group in pattern_type.values():
                self.all_patterns.update(length_group)
    
    def _is_valid_pattern(self, substring: str) -> bool:
        """
        Check if the substring is a valid DOLPHIN pattern.
        
        Args:
            substring (str): Substring to check
        
        Returns:
            bool: True if substring is a valid pattern, False otherwise
        """
        return substring in self.all_patterns
    
    def _get_pattern_type(self, substring: str) -> str:
        """
        Determine the type of pattern (vowel, consonant, or None)
        
        Args:
            substring (str): Substring to check
        
        Returns:
            str: Type of pattern ('vowel', 'consonant', or None)
        """
        for pattern_type, type_patterns in self.patterns.items():
            for length_group in type_patterns.values():
                if substring in length_group:
                    return pattern_type[:-1]  # Remove 's' from 'vowels'/'consonants'
        return None
    
    def construct_output_function(self, domain_name: str) -> List[Tuple[str, str]]:
        """
        Reconstruct the output function as described in Algorithm 1 of the paper.
        Implements a greedy matching approach with state-based output.
        
        Args:
            domain_name (str): Domain name to process
        
        Returns:
            List of tuples with matched substrings and their types
        """
        output = []
        i = 0
        length = len(domain_name)
        
        while i < length:
            # Try greedy matching from longest to shortest patterns
            match_found = False
            for pattern_length in range(min(self.max_pattern_length, length - i), 0, -1):
                substring = domain_name[i:i + pattern_length]
                
                if self._is_valid_pattern(substring):
                    pattern_type = self._get_pattern_type(substring)
                    output.append((substring, pattern_type))
                    i += pattern_length
                    match_found = True
                    break
            
            # If no pattern match, add individual character
            if not match_found:
                output.append((domain_name[i], 'neutral'))
                i += 1
        
        return output
    
    def matching_function(self, domain_name: str) -> List[Tuple[str, str]]:
        """
        Implements the matching function as described in Algorithm 2 of the paper.
        
        Args:
            domain_name (str): Domain name to process
        
        Returns:
            List of tuples with matched substrings and their types
        """
        # This method can be expanded to more closely match the paper's state machine
        # For now, it leverages the construct_output_function
        return self.construct_output_function(domain_name)

# feature_extraction.py

from typing import List, Tuple, Dict

def compute_phonics_features(matches: List[Tuple[str, str]]) -> Dict[str, float]:
    """
    Extract phonics-based features from DOLPHIN pattern matching.
    
    Args:
        matches (List[Tuple[str, str]]): List of matched substrings with their types
    
    Returns:
        Dict[str, float]: Extracted phonics features
    """
    # Initialize feature dictionary
    features = {
        'vowel_ratio': 0.0,
        'consonant_ratio': 0.0,
        'neutral_ratio': 0.0,
        'pattern_diversity': 0.0,
        'max_pattern_length': 0.0
    }
    
    # If no matches, return default features
    if not matches:
        return features
    
    # Compute feature calculations
    total_matches = len(matches)
    
    # Compute type ratios
    type_counts = {}
    max_pattern_length = 0
    
    for pattern, pattern_type in matches:
        # Track pattern type counts
        type_counts[pattern_type] = type_counts.get(pattern_type, 0) + 1
        
        # Track max pattern length
        max_pattern_length = max(max_pattern_length, len(pattern))
    
    # Calculate ratios
    features['vowel_ratio'] = type_counts.get('vowel', 0) / total_matches
    features['consonant_ratio'] = type_counts.get('consonant', 0) / total_matches
    features['neutral_ratio'] = type_counts.get('neutral', 0) / total_matches
    
    # Pattern diversity (number of unique pattern types)
    features['pattern_diversity'] = len(set(p for _, p in matches)) / total_matches
    
    # Max pattern length
    features['max_pattern_length'] = max_pattern_length
    
    return features

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

def train_and_evaluate_model(features_df: pd.DataFrame, unseen_data_path=None):
    # Separate features and labels
    X = features_df.drop('label', axis=1)
    y = features_df['label']
    
    # Create a pipeline for scaling and classification
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            class_weight='balanced',
            max_depth=10,
            min_samples_split=5
        ))
    ])
    
    # Perform 5-fold cross-validation
    cross_val_scores = cross_val_score(pipeline, X, y, cv=5)
    print("Cross-validation scores:", cross_val_scores)
    print("Average cross-validation score:", np.mean(cross_val_scores))
    
    # Train-test split for final evaluation
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    
    # Detailed metrics
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    # FPR and FNR
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    fpr = fp / (fp + tn)
    fnr = fn / (tp + fn)
    print(f"\nAccuracy: {accuracy:.4f}")
    print(f"False Positive Rate (FPR): {fpr:.4f}")
    print(f"False Negative Rate (FNR): {fnr:.4f}")
    
    # Feature importance
    rf_classifier = pipeline.named_steps['classifier']
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': rf_classifier.feature_importances_
    }).sort_values(by='importance', ascending=False)
    print("\nFeature Importance:")
    print(feature_importance)
    
# preprocess.py

import pandas as pd
from typing import Set

def preprocess_dataset(dataset_path: str, whitelist: Set[str]) -> pd.DataFrame:
    """
    Preprocess the dataset by filtering and cleaning domain data.
    
    Args:
        dataset_path (str): Path to the input dataset
        whitelist (Set[str]): Set of whitelisted domains
    
    Returns:
        pd.DataFrame: Preprocessed dataset with filtered domains
    """
    try:
        # Read the dataset with header handling to ensure correct column names
        data = pd.read_csv(dataset_path, header=0)  # header=0 tells pandas to use the first row as header
    except Exception as e:
        print(f"Error reading dataset: {e}")
        raise
    
    # Ensure the dataset contains the expected columns
    if "domain_name" not in data.columns or "label" not in data.columns:
        print("Dataset missing expected columns. Assigning default headers.")
        data.columns = ["domain_name", "label"]  # In case columns are not correctly set
    
    # Check if 'label' column exists and assign default labels if missing
    if data['label'].isnull().all():
        print("No labels found in dataset. Assigning default labels (0).")
        data['label'] = 0  # Default label assignment

    # Preprocess domain names (strip whitespace and convert to lowercase)
    data['domain_name'] = data['domain_name'].str.lower().str.strip()

    # Filter out domains in the whitelist
    data = data[~data['domain_name'].isin(whitelist)]

    # Remove any rows with missing values in either 'domain_name' or 'label'
    data = data.dropna(subset=['domain_name', 'label'])

    return data
